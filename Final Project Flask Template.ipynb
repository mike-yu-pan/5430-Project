{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adff3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" !pip install -U pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e71aa4",
   "metadata": {},
   "source": [
    "# StreamQuest Movie Plot Searching System\n",
    "This application is created for movie studios to check the latest trends in the movie industry......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f893e",
   "metadata": {},
   "source": [
    "## Section 1. Word2vec Movie Recommender Model Training\n",
    "The first part of this notebook is dedicated to data cleaning and trainning the word2vec model to create the following three tools for the studio writers and executives: \n",
    "1. Basic movie recommender: The user input one movie; and the system recommends 10 other movies with similar plotlines. \n",
    "2. Advance movie recommender: The user input two movies; and the system recommends 10 other movies with plotlines that are similar to the combination of these two movies. \n",
    "3. Duplicate plot checker: The user input his/her script for a new movie idea, and the system checks if his/her idea has already been produced in a previous movie. \n",
    "\n",
    "### 1.1 Data cleaning\n",
    "There are three data sources used in this section: \n",
    "1. IMDB: used for matching movie title & ID\n",
    "2. Details: contains plots&movie ID, used for trainning\n",
    "3. Wiki_Plot: contains plots& movie name, used for trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a81153",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_imdb_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/title.basics.tsv.gz'\n",
    "path_to_reviews_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/IMDB_reviews.json'\n",
    "path_to_plots_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/wiki_movie_plots_deduped.csv'\n",
    "path_to_details_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/IMDB_movie_details.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd85faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Using Apache Spark Version\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055e8fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean & combine the IMDB dataset with details dataset \n",
    "# reading the IMDB dataset\n",
    "imdb = spark.read.options(header = True, inferSchema = True, delimiter = \"\\t\")\\\n",
    "  .csv(path_to_imdb_dataset)\n",
    "# filter the imdb dataset so that only movies are included\n",
    "imdb = imdb.filter(\"titleType = 'movie'\")\\\n",
    "  .select('tconst', 'primaryTitle', 'startYear')\\\n",
    "    .withColumnRenamed('startYear', 'Year')\\\n",
    "      .withColumnRenamed('primaryTitle', 'Title')\\\n",
    "        .dropDuplicates(['Title', 'Year'])\n",
    "\n",
    "\n",
    "# reading the details dataset, preserving only three important variables\n",
    "details_summary = spark.read.json(path_to_details_dataset)\n",
    "details_summary = details_summary\\\n",
    "  .select('movie_id','plot_summary')\\\n",
    "    .withColumnRenamed('plot_summary','Plot')\n",
    "\n",
    "# reading the details dataset, preserving only three important variables\n",
    "details_synopsis = spark.read.json(path_to_details_dataset)\n",
    "details_synopsis = details_synopsis.select('movie_id','plot_synopsis')\\\n",
    "  .filter(\"plot_synopsis != ''\")\\\n",
    "    .withColumnRenamed('plot_synopsis', 'Plot')\n",
    "\n",
    "details = details_summary.union(details_synopsis)\n",
    "print('there is a total of ', details.count(), ' plot descriptions in the details dataset')\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "# join the imdb with details by matching the unique identifier(e.g. tt0000000)\n",
    "imdb_join_details = imdb.join(details, imdb.tconst == details.movie_id, 'inner')\\\n",
    "  .withColumnRenamed('tconst', 'id')\\\n",
    "    .select('id', 'Title', 'Plot')\\\n",
    "      .withColumn(\"Source\", lit(\"imdb_details\"))\n",
    "\n",
    "print(\"The joined dataset has \", imdb_join_details.count(), \" entries\")\n",
    "# inspect the joined dataset\n",
    "imdb_join_details.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a785ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and combine wiki plot dataset with imdb dataset\n",
    "from pyspark.sql.functions import length\n",
    "# reading the plot dataset, preserving only three important variables\n",
    "wiki_plot = spark.read.options(header = True, inferSchema = True, quote = '\"', escape = '\"', multiLine = True).csv(path_to_plots_dataset)\n",
    "wiki_plot = wiki_plot.select('Title', 'Release Year','Plot')\\\n",
    "  .withColumnRenamed('Release Year', 'Year')\\\n",
    "    .filter(length(wiki_plot['Plot']) >= 200) # filter out the very short plot descriptions\n",
    "\n",
    "\n",
    "# join the imdb with the plot dataset by matching movie titles and release year\n",
    "imdb_join_plot = imdb.join(wiki_plot, [\"Title\", \"Year\"], 'inner')\\\n",
    "  .withColumnRenamed('tconst', 'id')\\\n",
    "    .select('id', 'Title', 'Plot')\\\n",
    "      .withColumn(\"Source\", lit(\"wiki_plot\"))\n",
    "\n",
    "print(\"The joined dataset has \", imdb_join_plot.count(), \" entries\")\n",
    "# inspect the joined dataset\n",
    "imdb_join_plot.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad0ae58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the above two dataset to get the dataset that we will train the model on\n",
    "df = imdb_join_plot.union(imdb_join_details)\n",
    "\n",
    "print('after merging & cleaning, there is a total of ', df.count(), ' movie plot entries left in the merged dataset')\n",
    "# inspect the combined new dataset\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf51036",
   "metadata": {},
   "source": [
    "### 1.2 Trainning Word2vec Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5809ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and remove stop words in this cell\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec\n",
    "\n",
    "# create a new field by copying Plot\n",
    "df = df.withColumn('inputText', F.col('Plot')) \n",
    "\n",
    "# regular expression tokenizer to tokenize inputText into individual tokens (words)\n",
    "regextok = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'inputText', outputCol = 'tokens')\n",
    "\n",
    "# StopWordsRemover to remove stopwords in the list of tokens\n",
    "stopwrmv = StopWordsRemover(inputCol = 'tokens', outputCol = 'tokens_sw_removed')\n",
    "df = regextok.transform(df)\n",
    "df = stopwrmv.transform(df)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7e668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model, the parameters here can be changed to optimize the model\n",
    "word2vec = Word2Vec(vectorSize = 100, minCount = 5, inputCol = 'tokens_sw_removed', outputCol = 'wordvectors')\n",
    "model = word2vec.fit(df)\n",
    "\n",
    "# using transform to add wordvectors column to dataframe\n",
    "df = model.transform(df)\n",
    "chunks = df.select('id', 'Title','wordvectors', 'Plot', 'Source').limit(30000).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98e1a0",
   "metadata": {},
   "source": [
    "### 1.3 Create Basic Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a function to obtain the plot string from the plot dataset\n",
    "def acquire_plot(base_movie: str): \n",
    "  # input: a movie name (precise) or a movie id \n",
    "  # output: the movie's plot\n",
    "\n",
    "  if base_movie.startswith(\"tt\"):   # search by movie name\n",
    "    base_movie_row = df.filter(df.id == base_movie).collect()\n",
    "  else:                             # search by movie id\n",
    "    base_movie_row = df.filter(df.Title == base_movie).collect()\n",
    "\n",
    "  if base_movie_row: \n",
    "    movie_plot = base_movie_row[0]['Plot']\n",
    "    return movie_plot\n",
    "  else: \n",
    "    print(\"Sorry, \", base_movie, \" is not found in the database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac80ffe5",
   "metadata": {},
   "source": [
    "### 1.4 Create Advanced Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1077668a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2e5a73d",
   "metadata": {},
   "source": [
    "### 1.5 Create Duplicate Plot Checker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7969763b",
   "metadata": {},
   "source": [
    "### a few things to pay attention to: \n",
    "1. the data takes a few minutes to clean and the word2vec takes 2.5 minutes to train(aparently there is a way to save the trained model, I'll figure it out ASAP), so we can implement the cleaning code before the flask code so that it can be run before we do the demo\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3708ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Search Engine Template -> uses spark df to query based on user input\n",
    "### need to different html -> one is the home page (user input) then the second one is the output screen\n",
    "#### there is a way to only use one html for simplicity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify, redirect, url_for, render_template\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(\"JSON_OUTPUT\")\n",
    "\n",
    "@app.route('/')\n",
    "def form():\n",
    "    return render_template('moreedits.html')\n",
    "        \n",
    "@app.route('/submit', methods=['GET','POST'])\n",
    "def submit():\n",
    "    if request.method == 'POST':\n",
    "        types_user = request.form['q']\n",
    "        price_level_user = request.form['price_level']\n",
    "        neighborhood_user = request.form['neighborhoods']\n",
    "        rating_user = request.form['rating']\n",
    "        \n",
    "        def cossim(v1, v2):\n",
    "            dot_product = np.sum(v1 * v2)\n",
    "            mag_v1 = np.sqrt(np.sum(np.power(v1, 2)))\n",
    "            mag_v2 = np.sqrt(np.sum(np.power(v2, 2)))\n",
    "            return dot_product / (mag_v1 * mag_v2 + 0.1)\n",
    "\n",
    "        query_txt = types_user\n",
    "        query_df = sc.parallelize([(1,query_txt)]).toDF(['index','Types'])\n",
    "        query_tok = regexTokFilter.transform(query_df)\n",
    "        query_vec = model_type.transform(query_tok)\n",
    "        query_vec = query_vec.select('wordvectors_type').collect()[0][0]\n",
    "        \n",
    "        sim_rdd = sc.parallelize((i[0],i[1], i[2],i[3],i[4],i[5],i[6],float(cossim(query_vec, i[7])), i[8]) for i in sparkDF_wv_final)\n",
    "        sim_df = spark.createDataFrame(sim_rdd).\\\n",
    "            withColumnRenamed('_1', 'Name').\\\n",
    "            withColumnRenamed('_2', 'Address').\\\n",
    "            withColumnRenamed('_3', 'Latitude').\\\n",
    "            withColumnRenamed('_4', 'Longitude').\\\n",
    "            withColumnRenamed('_5', 'Types').\\\n",
    "            withColumnRenamed('_6', 'Price_level').\\\n",
    "            withColumnRenamed('_7', 'Rating').\\\n",
    "            withColumnRenamed('_8', 'Similarity').\\\n",
    "            withColumnRenamed('_9', 'Neighborhood').\\\n",
    "            orderBy(\"Similarity\", ascending=False)\n",
    "\n",
    "    \n",
    "    pandas_df = sim_df.toPandas()\n",
    "    df_filtered = pandas_df[pandas_df['Price_level'] == int(price_level_user)]\n",
    "    df_filtered1 = df_filtered[df_filtered['Rating'] == int(rating_user)]\n",
    "    df_filtered2 = df_filtered1[df_filtered1['Neighborhood'] == neighborhood_user]\n",
    "    html_table = df_filtered2.head(10).to_html(classes='table')\n",
    "    return render_template('onlytable.html', table=html_table)\n",
    "\n",
    "@app.route('/output')\n",
    "def output():\n",
    "    # render the output HTML page\n",
    "    return render_template('onlytable.html',table=processed_data)\n",
    "\n",
    "app.run(host='localhost', port=7030)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb6087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
