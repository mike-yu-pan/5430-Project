{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adff3969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" !pip install -U pyspark\\nfrom pyspark.sql import SparkSession\\nimport os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" !pip install -U pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e71aa4",
   "metadata": {},
   "source": [
    "# StreamQuest Movie Plot Searching System\n",
    "This application is created for movie studios to check the latest trends in the movie industry......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f893e",
   "metadata": {},
   "source": [
    "## Section 1. Word2vec Movie Recommender Model Training\n",
    "The first part of this notebook is dedicated to data cleaning and trainning the word2vec model to create the following three tools for the studio writers and executives: \n",
    "1. Basic movie recommender: The user input one movie; and the system recommends 10 other movies with similar plotlines. \n",
    "2. Advance movie recommender: The user input two movies; and the system recommends 10 other movies with plotlines that are similar to the combination of these two movies. \n",
    "3. Duplicate plot checker: The user input his/her script for a new movie idea, and the system checks if his/her idea has already been produced in a previous movie. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11267477",
   "metadata": {},
   "source": [
    "### 1.1 Data cleaning\n",
    "There are three data sources used in this section: \n",
    "1. IMDB: used for matching movie name and ID\n",
    "2. Details: contains plot summaries, synopsis and movie ID, both summaries and synopsis will be used for trainning\n",
    "3. Wiki_Plot: contains plots and movie name, used for trainning\n",
    "\n",
    "Note that there might be multiple entries for a same movie because multiple authors might have written summaries for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a81153",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_imdb_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/title.basics.tsv.gz'\n",
    "path_to_plots_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/wiki_movie_plots_deduped.csv'\n",
    "path_to_details_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/IMDB_movie_details.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd85faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/10 08:52:33 WARN Utils: Your hostname, Yus-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 192.168.181.65 instead (on interface en0)\n",
      "23/08/10 08:52:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/10 08:52:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apache Spark Version 3.4.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Using Apache Spark Version\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055e8fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a total of  637758  movies in the imdb dataset\n",
      "there is a total of  2911  plot descriptions in the details dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joined dataset has  2857  entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+------------+\n",
      "|       id|         Title|                Plot|      Source|\n",
      "+---------+--------------+--------------------+------------+\n",
      "|tt2294449|22 Jump Street|Following their s...|imdb_details|\n",
      "|tt2294449|22 Jump Street|After making thei...|imdb_details|\n",
      "|tt0120623|  A Bug's Life|On a small island...|imdb_details|\n",
      "+---------+--------------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# clean & combine the IMDB dataset with details dataset \n",
    "# reading the IMDB dataset\n",
    "imdb = spark.read.options(header = True, inferSchema = True, delimiter = \"\\t\")\\\n",
    "  .csv(path_to_imdb_dataset)\n",
    "# filter the imdb dataset so that only movies are included\n",
    "imdb = imdb.filter(\"titleType = 'movie'\")\\\n",
    "  .select('tconst', 'primaryTitle', 'startYear')\\\n",
    "    .withColumnRenamed('startYear', 'Year')\\\n",
    "      .withColumnRenamed('primaryTitle', 'Title')\\\n",
    "        .dropDuplicates(['Title', 'Year'])\n",
    "print('there is a total of ', imdb.count(), ' movies in the imdb dataset')\n",
    "\n",
    "\n",
    "# reading the details dataset, preserving only three important variables\n",
    "details_summary = spark.read.json(path_to_details_dataset)\n",
    "details_summary = details_summary\\\n",
    "  .select('movie_id','plot_summary')\\\n",
    "    .withColumnRenamed('plot_summary','Plot')\n",
    "\n",
    "# reading the details dataset, preserving only three important variables\n",
    "details_synopsis = spark.read.json(path_to_details_dataset)\n",
    "details_synopsis = details_synopsis.select('movie_id','plot_synopsis')\\\n",
    "  .filter(\"plot_synopsis != ''\")\\\n",
    "    .withColumnRenamed('plot_synopsis', 'Plot')\n",
    "\n",
    "details = details_summary.union(details_synopsis)\n",
    "print('there is a total of ', details.count(), ' plot descriptions in the details dataset')\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "# join the imdb with details by matching the unique identifier(e.g. tt0000000)\n",
    "imdb_join_details = imdb.join(details, imdb.tconst == details.movie_id, 'inner')\\\n",
    "  .withColumnRenamed('tconst', 'id')\\\n",
    "    .select('id', 'Title', 'Plot')\\\n",
    "      .withColumn(\"Source\", lit(\"imdb_details\"))\n",
    "\n",
    "print(\"The joined dataset has \", imdb_join_details.count(), \" entries\")\n",
    "# inspect the joined dataset\n",
    "imdb_join_details.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a785ac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a total of  33243  plot descriptions in the wiki_plot dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joined dataset has  25361  entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+\n",
      "|       id|Title|                Plot|   Source|\n",
      "+---------+-----+--------------------+---------+\n",
      "|tt0790799|$9.99|The film mainly f...|wiki_plot|\n",
      "+---------+-----+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# clean and combine wiki plot dataset with imdb dataset\n",
    "from pyspark.sql.functions import length\n",
    "# reading the plot dataset, preserving only three important variables\n",
    "wiki_plot = spark.read.options(header = True, inferSchema = True, quote = '\"', escape = '\"', multiLine = True).csv(path_to_plots_dataset)\n",
    "wiki_plot = wiki_plot.select('Title', 'Release Year','Plot')\\\n",
    "  .withColumnRenamed('Release Year', 'Year')\\\n",
    "    .filter(length(wiki_plot['Plot']) >= 200) # filter out the very short plot descriptions\n",
    "print('there is a total of ', wiki_plot.count(), ' plot descriptions in the wiki_plot dataset')\n",
    "\n",
    "\n",
    "# join the imdb with the plot dataset by matching movie titles and release year\n",
    "imdb_join_plot = imdb.join(wiki_plot, [\"Title\", \"Year\"], 'inner')\\\n",
    "  .withColumnRenamed('tconst', 'id')\\\n",
    "    .select('id', 'Title', 'Plot')\\\n",
    "      .withColumn(\"Source\", lit(\"wiki_plot\"))\n",
    "\n",
    "print(\"The joined dataset has \", imdb_join_plot.count(), \" entries\")\n",
    "# inspect the joined dataset\n",
    "imdb_join_plot.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad0ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merging & cleaning, there is a total of  28218  movie plot entries left in the merged dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+\n",
      "|       id|Title|                Plot|   Source|\n",
      "+---------+-----+--------------------+---------+\n",
      "|tt0790799|$9.99|The film mainly f...|wiki_plot|\n",
      "+---------+-----+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# combine the above two dataset to get the dataset that we will train the model on\n",
    "df = imdb_join_plot.union(imdb_join_details)\n",
    "\n",
    "print('after merging & cleaning, there is a total of ', df.count(), ' movie plot entries left in the merged dataset')\n",
    "# inspect the combined new dataset\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf51036",
   "metadata": {},
   "source": [
    "### 1.2 Trainning Word2vec Model \n",
    "This Word2Vec model is trained using the cleaned data above, the model is fed with around 28,000 entries of texts that describe movie plots. The resulting model will be useful in finding similarities in movie plotlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5809ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|       id|Title|                Plot|   Source|           inputText|              tokens|   tokens_sw_removed|\n",
      "+---------+-----+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|tt0790799|$9.99|The film mainly f...|wiki_plot|The film mainly f...|[the, film, mainl...|[film, mainly, fo...|\n",
      "+---------+-----+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# tokenize and remove stop words in this cell\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec\n",
    "\n",
    "# create a new field by copying Plot\n",
    "df = df.withColumn('inputText', F.col('Plot')) \n",
    "\n",
    "# regular expression tokenizer to tokenize inputText into individual tokens (words)\n",
    "regextok = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'inputText', outputCol = 'tokens')\n",
    "\n",
    "# StopWordsRemover to remove stopwords in the list of tokens\n",
    "stopwrmv = StopWordsRemover(inputCol = 'tokens', outputCol = 'tokens_sw_removed')\n",
    "df = regextok.transform(df)\n",
    "df = stopwrmv.transform(df)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7e668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/10 08:55:40 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train word2vec model, the parameters here can be changed to optimize the model\n",
    "word2vec = Word2Vec(vectorSize = 100, minCount = 5, inputCol = 'tokens_sw_removed', outputCol = 'wordvectors')\n",
    "model = word2vec.fit(df)\n",
    "\n",
    "# using transform to add wordvectors column to dataframe\n",
    "df = model.transform(df)\n",
    "chunks = df.select('id', 'Title','wordvectors', 'Plot', 'Source').limit(30000).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a433ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate cosine similarity for later\n",
    "import numpy as np\n",
    "def cossim(v1, v2): \n",
    "  '''\n",
    "      cossim(v1, v2) calculates the cosine similarity between v1 and v1.\n",
    "      If v1 or v2 is a zero vector, it will return 0\n",
    "  '''\n",
    "  if np.dot(v1, v1) == 0 or np.dot(v2, v2) == 0:\n",
    "      return 0.0\n",
    "  return float(np.dot(v1, v2) / np.sqrt(np.dot(v1, v1)) / (np.sqrt(np.dot(v2, v2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98e1a0",
   "metadata": {},
   "source": [
    "### 1.3 Create Basic, Advanced Recommender and Duplicate Plot Checker\n",
    "These three tools use the same model at their core. Therefore, to optimize performance, the implementation will create only a single session when running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd10102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a function to obtain the plot string from the plot dataset\n",
    "def acquire_plot(base_movie: str): \n",
    "  # input: a movie name (precise) or a movie id \n",
    "  # output: the movie's plot\n",
    "\n",
    "  if base_movie.startswith(\"tt\"):   # search by movie name\n",
    "    base_movie_row = df.filter(df.id == base_movie).collect()\n",
    "  else:                             # search by movie id\n",
    "    base_movie_row = df.filter(df.Title == base_movie).collect()\n",
    "\n",
    "  if base_movie_row: \n",
    "    movie_plot = base_movie_row[0]['Plot']\n",
    "    return movie_plot\n",
    "  else: \n",
    "    print(\"Sorry, \", base_movie, \" is not found in the database. Please type in exact movie names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4973a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preprocessing(plot: str): \n",
    "  plot_df = spark.createDataFrame([(1, plot)]).toDF('index','inputText')\n",
    "  plot_tok = regextok.transform(plot_df)\n",
    "  plot_swr = stopwrmv.transform(plot_tok)\n",
    "  plot_vec = model.transform(plot_swr)\n",
    "  plot_vec = plot_vec.select('wordvectors').collect()[0][0]\n",
    "  return plot_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb45e02",
   "metadata": {},
   "source": [
    "### 1.3.1 Basic recommender\n",
    "The Basic Recommender only takes in one movie as the only parameter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "717173d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_user_1 = '2012'    # User input The exact movie name or movie id(e.g. 'tt1023003')\n",
    "basic_movie_plot = acquire_plot(input_user_1)\n",
    "basic_vec = query_preprocessing(basic_movie_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b538bcc",
   "metadata": {},
   "source": [
    "### 1.3.2 Advanced Recommender\n",
    "The Advanced Recommender takes in one extra movie as the second parameter, then our word2vec model will be able to recommend a third movie that has similar plot as the combination of the first two. : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39f4126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_user_2 = 'tt0468569' #'tt0468569'    # User input The exact movie name or movie id(e.g. 'tt1023003')\n",
    "if input_user_2: \n",
    "  second_movie_plot = acquire_plot(input_user_2)\n",
    "  second_vec = query_preprocessing(second_movie_plot)\n",
    "  combined_vec = basic_vec + second_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c02b5",
   "metadata": {},
   "source": [
    "### 1.3.3 Duplicate Plot Checker\n",
    "The duplicate Plot checker is similar to the basic recommender where it finds an existing movie with similar plot as the user's input plot description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ed4012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user_3 = 'Once upon a time in a crime ridden Gotham City, a member of the rich Wayne family decided to put on a mask and protect the people of Gotham'\n",
    "check_vec = query_preprocessing(input_user_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13ed06",
   "metadata": {},
   "source": [
    "### 1.4 implementation of the above three tools\n",
    "The parameters of the three tools have been created. To optimize performance, the implementation will create only one single session when running. \n",
    "\n",
    "__Note for Meenu__ : the order of the IF clauses in this following cell is important, the user can only run one of the three tools at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1c628e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Duplicate Plot Checker\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRunning Duplicate Plot Checker\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m data \u001b[39m=\u001b[39m [(i[\u001b[39m0\u001b[39m], \u001b[39mfloat\u001b[39m(cossim(check_vec, i[\u001b[39m2\u001b[39m])), i[\u001b[39m1\u001b[39m], i[\u001b[39m4\u001b[39m], i[\u001b[39m3\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m chunks]\n\u001b[0;32m----> 5\u001b[0m sim_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mcreateDataFrame(data)\u001b[39m.\u001b[39mtoDF(\u001b[39m'\u001b[39m\u001b[39mmovie_id\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mSource\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPlot\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m sim_df \u001b[39m=\u001b[39m (sim_df\u001b[39m.\u001b[39mdropDuplicates([\u001b[39m'\u001b[39m\u001b[39mmovie_id\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m           \u001b[39m.\u001b[39morderBy(\u001b[39m'\u001b[39m\u001b[39msimilarity\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m           \u001b[39m.\u001b[39mlimit(\u001b[39m30\u001b[39m))\n\u001b[1;32m      9\u001b[0m sim_df\u001b[39m.\u001b[39mshow(\u001b[39m10\u001b[39m, truncate\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/session.py:1222\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[0;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[1;32m   1220\u001b[0m SparkSession\u001b[39m.\u001b[39m_activeSession \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1221\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1222\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mSparkSession\u001b[39m.\u001b[39msetActiveSession(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsparkSession)\n\u001b[1;32m   1223\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, DataFrame):\n\u001b[1;32m   1224\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdata is already a DataFrame\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39m==\u001b[39m UserHelpAutoCompletion\u001b[39m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[39mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client\u001b[39m.\u001b[39;49msend_command(\n\u001b[1;32m   1713\u001b[0m     proto\u001b[39m.\u001b[39;49mREFLECTION_COMMAND_NAME \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m     proto\u001b[39m.\u001b[39;49mREFL_GET_UNKNOWN_SUB_COMMAND_NAME \u001b[39m+\u001b[39;49m name \u001b[39m+\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_id \u001b[39m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m proto\u001b[39m.\u001b[39;49mEND_COMMAND_PART)\n\u001b[1;32m   1716\u001b[0m \u001b[39mif\u001b[39;00m answer \u001b[39m==\u001b[39m proto\u001b[39m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[39mreturn\u001b[39;00m JavaPackage(name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client, jvm_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_id)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msend_command\u001b[39m(\u001b[39mself\u001b[39m, command, retry\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[39m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[39m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[39m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_connection()\n\u001b[1;32m   1037\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m connection \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m connection\u001b[39m.\u001b[39msocket \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_new_connection()\n\u001b[1;32m    285\u001b[0m \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_create_new_connection\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[39m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_parameters, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_property, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     connection\u001b[39m.\u001b[39;49mconnect_to_java_server()\n\u001b[1;32m    292\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[39mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mssl_context\u001b[39m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket, server_hostname\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msocket\u001b[39m.\u001b[39;49mconnect((\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_address, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mjava_port))\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msocket\u001b[39m.\u001b[39mmakefile(\u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connected \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "if input_user_3: \n",
    "  \n",
    "  print(\"Running Duplicate Plot Checker\")\n",
    "  data = [(i[0], float(cossim(check_vec, i[2])), i[1], i[4], i[3]) for i in chunks]\n",
    "  sim_df = spark.createDataFrame(data).toDF('movie_id', 'similarity', 'Title', 'Source', 'Plot')\n",
    "  sim_df = (sim_df.dropDuplicates(['movie_id'])\n",
    "            .orderBy('similarity', ascending=False)\n",
    "            .limit(30))\n",
    "  sim_df.show(10, truncate=False)\n",
    "\n",
    "elif not input_user_2:  # if input_user_2 is empty, then run the basic recommender\n",
    "  \n",
    "  print(\"Only one movie is input, running Basic Recommender\")\n",
    "  data = [(i[0], float(cossim(basic_vec, i[2])), i[1], i[4], i[3]) for i in chunks]\n",
    "  sim_df = spark.createDataFrame(data).toDF('movie_id', 'similarity', 'Title', 'Source', 'Plot')\n",
    "  sim_df = (sim_df.filter((sim_df.Title != input_user_1) & (sim_df.movie_id != input_user_1))\n",
    "            .dropDuplicates(['movie_id'])\n",
    "            .orderBy('similarity', ascending=False)\n",
    "            .limit(30))\n",
    "  sim_df.show(10, truncate=False)\n",
    "  \n",
    "elif input_user_2: \n",
    "  \n",
    "  print(\"Two movies are input, running Advanced Recommender\")\n",
    "  data = [(i[0], float(cossim(combined_vec, i[2])), i[1], i[4], i[3]) for i in chunks]\n",
    "  sim_df = spark.createDataFrame(data).toDF('movie_id', 'similarity', 'Title', 'Source', 'Plot')\n",
    "  sim_df = (sim_df.filter((sim_df.Title != input_user_1) & (sim_df.movie_id != input_user_1)\n",
    "                          & (sim_df.Title != input_user_2) & (sim_df.movie_id != input_user_2))\n",
    "            .dropDuplicates(['movie_id'])\n",
    "            .orderBy('similarity', ascending=False)\n",
    "            .limit(30))\n",
    "  sim_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb6087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
