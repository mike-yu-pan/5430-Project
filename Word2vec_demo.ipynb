{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adff3969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" !pip install -U pyspark\\nfrom pyspark.sql import SparkSession\\nimport os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" !pip install -U pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e71aa4",
   "metadata": {},
   "source": [
    "# StreamQuest Movie Plot Searching System\n",
    "This application is created for movie studios to check the latest trends in the movie industry......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f893e",
   "metadata": {},
   "source": [
    "## Section 1. Word2vec Movie Recommender Model Training\n",
    "The first part of this notebook is dedicated to data cleaning and trainning the word2vec model to create the following three tools for the studio writers and executives: \n",
    "1. Basic movie recommender: The user input one movie; and the system recommends 10 other movies with similar plotlines. \n",
    "2. Advance movie recommender: The user input two movies; and the system recommends 10 other movies with plotlines that are similar to the combination of these two movies. \n",
    "3. Duplicate plot checker: The user input his/her script for a new movie idea, and the system checks if his/her idea has already been produced in a previous movie. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11267477",
   "metadata": {},
   "source": [
    "### 1.1 Data cleaning\n",
    "There are three data sources used in this section: \n",
    "1. IMDB: used for matching movie name and ID\n",
    "2. Details: contains plot summaries, synopsis and movie ID, both summaries and synopsis will be used for trainning\n",
    "3. Wiki_Plot: contains plots and movie name, used for trainning\n",
    "\n",
    "Note that there might be multiple entries for a same movie because multiple authors might have written summaries for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a81153",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_imdb_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/title.basics.tsv.gz'\n",
    "path_to_plots_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/wiki_movie_plots_deduped.csv'\n",
    "path_to_details_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/IMDB_movie_details.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd85faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/10 16:24:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apache Spark Version 3.4.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Using Apache Spark Version\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055e8fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a total of  637758  movies in the imdb dataset\n",
      "there is a total of  2911  plot descriptions in the details dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joined dataset has  2857  entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+------------+\n",
      "|       id|         Title|                Plot|      Source|\n",
      "+---------+--------------+--------------------+------------+\n",
      "|tt2294449|22 Jump Street|Following their s...|imdb_details|\n",
      "|tt2294449|22 Jump Street|After making thei...|imdb_details|\n",
      "|tt0120623|  A Bug's Life|On a small island...|imdb_details|\n",
      "+---------+--------------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# clean & combine the IMDB dataset with details dataset \n",
    "# reading the IMDB dataset\n",
    "imdb = spark.read.options(header = True, inferSchema = True, delimiter = \"\\t\")\\\n",
    "  .csv(path_to_imdb_dataset)\n",
    "# filter the imdb dataset so that only movies are included\n",
    "imdb = imdb.filter(\"titleType = 'movie'\")\\\n",
    "  .select('tconst', 'primaryTitle', 'startYear')\\\n",
    "    .withColumnRenamed('startYear', 'Year')\\\n",
    "      .withColumnRenamed('primaryTitle', 'Title')\\\n",
    "        .dropDuplicates(['Title', 'Year'])\n",
    "print('there is a total of ', imdb.count(), ' movies in the imdb dataset')\n",
    "\n",
    "\n",
    "# reading the details dataset, preserving only three important variables\n",
    "details_summary = spark.read.json(path_to_details_dataset)\n",
    "details_summary = details_summary\\\n",
    "  .select('movie_id','plot_summary')\\\n",
    "    .withColumnRenamed('plot_summary','Plot')\n",
    "\n",
    "# reading the details dataset, preserving only three important variables\n",
    "details_synopsis = spark.read.json(path_to_details_dataset)\n",
    "details_synopsis = details_synopsis.select('movie_id','plot_synopsis')\\\n",
    "  .filter(\"plot_synopsis != ''\")\\\n",
    "    .withColumnRenamed('plot_synopsis', 'Plot')\n",
    "\n",
    "details = details_summary.union(details_synopsis)\n",
    "print('there is a total of ', details.count(), ' plot descriptions in the details dataset')\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "# join the imdb with details by matching the unique identifier(e.g. tt0000000)\n",
    "imdb_join_details = imdb.join(details, imdb.tconst == details.movie_id, 'inner')\\\n",
    "  .withColumnRenamed('tconst', 'id')\\\n",
    "    .select('id', 'Title', 'Plot')\\\n",
    "      .withColumn(\"Source\", lit(\"imdb_details\"))\n",
    "\n",
    "print(\"The joined dataset has \", imdb_join_details.count(), \" entries\")\n",
    "# inspect the joined dataset\n",
    "imdb_join_details.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a785ac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a total of  33243  plot descriptions in the wiki_plot dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joined dataset has  25361  entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+\n",
      "|       id|Title|                Plot|   Source|\n",
      "+---------+-----+--------------------+---------+\n",
      "|tt0790799|$9.99|The film mainly f...|wiki_plot|\n",
      "+---------+-----+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# clean and combine wiki plot dataset with imdb dataset\n",
    "from pyspark.sql.functions import length\n",
    "# reading the plot dataset, preserving only three important variables\n",
    "wiki_plot = spark.read.options(header = True, inferSchema = True, quote = '\"', escape = '\"', multiLine = True).csv(path_to_plots_dataset)\n",
    "wiki_plot = wiki_plot.select('Title', 'Release Year','Plot')\\\n",
    "  .withColumnRenamed('Release Year', 'Year')\\\n",
    "    .filter(length(wiki_plot['Plot']) >= 200) # filter out the very short plot descriptions\n",
    "print('there is a total of ', wiki_plot.count(), ' plot descriptions in the wiki_plot dataset')\n",
    "\n",
    "\n",
    "# join the imdb with the plot dataset by matching movie titles and release year\n",
    "imdb_join_plot = imdb.join(wiki_plot, [\"Title\", \"Year\"], 'inner')\\\n",
    "  .withColumnRenamed('tconst', 'id')\\\n",
    "    .select('id', 'Title', 'Plot')\\\n",
    "      .withColumn(\"Source\", lit(\"wiki_plot\"))\n",
    "\n",
    "print(\"The joined dataset has \", imdb_join_plot.count(), \" entries\")\n",
    "# inspect the joined dataset\n",
    "imdb_join_plot.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad0ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merging & cleaning, there is a total of  28218  movie plot entries left in the merged dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+\n",
      "|       id|Title|                Plot|   Source|\n",
      "+---------+-----+--------------------+---------+\n",
      "|tt0790799|$9.99|The film mainly f...|wiki_plot|\n",
      "+---------+-----+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# combine the above two dataset to get the dataset that we will train the model on\n",
    "df = imdb_join_plot.union(imdb_join_details)\n",
    "\n",
    "print('after merging & cleaning, there is a total of ', df.count(), ' movie plot entries left in the merged dataset')\n",
    "# inspect the combined new dataset\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf51036",
   "metadata": {},
   "source": [
    "### 1.2 Trainning Word2vec Model \n",
    "This Word2Vec model is trained using the cleaned data above, the model is fed with around 28,000 entries of texts that describe movie plots. The resulting model will be useful in finding similarities in movie plotlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5809ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|       id|Title|                Plot|   Source|           inputText|              tokens|   tokens_sw_removed|\n",
      "+---------+-----+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|tt0790799|$9.99|The film mainly f...|wiki_plot|The film mainly f...|[the, film, mainl...|[film, mainly, fo...|\n",
      "+---------+-----+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# tokenize and remove stop words in this cell\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec\n",
    "\n",
    "# create a new field by copying Plot\n",
    "df = df.withColumn('inputText', F.col('Plot')) \n",
    "\n",
    "# regular expression tokenizer to tokenize inputText into individual tokens (words)\n",
    "regextok = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'inputText', outputCol = 'tokens')\n",
    "\n",
    "# StopWordsRemover to remove stopwords in the list of tokens\n",
    "stopwrmv = StopWordsRemover(inputCol = 'tokens', outputCol = 'tokens_sw_removed')\n",
    "df = regextok.transform(df)\n",
    "df = stopwrmv.transform(df)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7e668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/10 16:27:08 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train word2vec model, the parameters here can be changed to optimize the model\n",
    "word2vec = Word2Vec(vectorSize = 100, minCount = 5, inputCol = 'tokens_sw_removed', outputCol = 'wordvectors')\n",
    "model = word2vec.fit(df)\n",
    "\n",
    "# using transform to add wordvectors column to dataframe\n",
    "df = model.transform(df)\n",
    "chunks = df.select('id', 'Title','wordvectors', 'Plot', 'Source').limit(30000).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a433ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate cosine similarity for later\n",
    "import numpy as np\n",
    "def cossim(v1, v2): \n",
    "  '''\n",
    "      cossim(v1, v2) calculates the cosine similarity between v1 and v1.\n",
    "      If v1 or v2 is a zero vector, it will return 0\n",
    "  '''\n",
    "  if np.dot(v1, v1) == 0 or np.dot(v2, v2) == 0:\n",
    "      return 0.0\n",
    "  return float(np.dot(v1, v2) / np.sqrt(np.dot(v1, v1)) / (np.sqrt(np.dot(v2, v2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98e1a0",
   "metadata": {},
   "source": [
    "### 1.3 Create Basic, Advanced Recommender and Duplicate Plot Checker\n",
    "These three tools use the same model at their core. Therefore, to optimize performance, the implementation will create only a single session when running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd10102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a function to obtain the plot string from the plot dataset\n",
    "def acquire_plot(base_movie: str): \n",
    "  # input: a movie name (precise) or a movie id \n",
    "  # output: the movie's plot\n",
    "\n",
    "  if base_movie.startswith(\"tt\"):   # search by movie name\n",
    "    base_movie_row = df.filter(df.id == base_movie).collect()\n",
    "  else:                             # search by movie id\n",
    "    base_movie_row = df.filter(df.Title == base_movie).collect()\n",
    "\n",
    "  if base_movie_row: \n",
    "    movie_plot = base_movie_row[0]['Plot']\n",
    "    return movie_plot\n",
    "  else: \n",
    "    print(\"Sorry, \", base_movie, \" is not found in the database. Please type in exact movie names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4973a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preprocessing(plot: str): \n",
    "  plot_df = spark.createDataFrame([(1, plot)]).toDF('index','inputText')\n",
    "  plot_tok = regextok.transform(plot_df)\n",
    "  plot_swr = stopwrmv.transform(plot_tok)\n",
    "  plot_vec = model.transform(plot_swr)\n",
    "  plot_vec = plot_vec.select('wordvectors').collect()[0][0]\n",
    "  return plot_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb45e02",
   "metadata": {},
   "source": [
    "### 1.3.1 Basic recommender\n",
    "The Basic Recommender only takes in one movie as the only parameter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717173d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_user_1 = '2012'    # User input The exact movie name or movie id(e.g. 'tt1023003')\n",
    "basic_movie_plot = acquire_plot(input_user_1)\n",
    "basic_vec = query_preprocessing(basic_movie_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b538bcc",
   "metadata": {},
   "source": [
    "### 1.3.2 Advanced Recommender\n",
    "The Advanced Recommender takes in one extra movie as the second parameter, then our word2vec model will be able to recommend a third movie that has similar plot as the combination of the first two. : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f4126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user_2 = '' #'tt0468569'    # User input The exact movie name or movie id(e.g. 'tt1023003')\n",
    "if input_user_2: \n",
    "  second_movie_plot = acquire_plot(input_user_2)\n",
    "  second_vec = query_preprocessing(second_movie_plot)\n",
    "  combined_vec = basic_vec + second_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c02b5",
   "metadata": {},
   "source": [
    "### 1.3.3 Duplicate Plot Checker\n",
    "The duplicate Plot checker is similar to the basic recommender where it finds an existing movie with similar plot as the user's input plot description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ed4012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user_3 = '' #'Once upon a time in a crime ridden Gotham City, a member of the rich Wayne family decided to put on a mask and protect the people of Gotham'\n",
    "check_vec = query_preprocessing(input_user_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13ed06",
   "metadata": {},
   "source": [
    "### 1.4 implementation of the above three tools\n",
    "The parameters of the three tools have been created. To optimize performance, the implementation will create only one single session when running. \n",
    "\n",
    "__Note for Meenu__ : the order of the IF clauses in this following cell is important, the user can only run one of the three tools at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c628e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only one movie is input, running Basic Recommender\n"
     ]
    }
   ],
   "source": [
    "output_lim = 10\n",
    "\n",
    "if input_user_3: \n",
    "  print(\"Running Duplicate Plot Checker\")\n",
    "  data = [(i[0], float(cossim(check_vec, i[2])), i[1], i[4], i[3]) for i in chunks]\n",
    "  sim_df = spark.createDataFrame(data).toDF('movie_id', 'similarity', 'Title', 'Source', 'Plot')\n",
    "  sim_df = (sim_df.dropDuplicates(['movie_id'])\n",
    "            .orderBy('similarity', ascending=False)\n",
    "            .limit(output_lim))\n",
    "\n",
    "elif not input_user_2:  # if input_user_2 is empty, then run the basic recommender\n",
    "  \n",
    "  print(\"Only one movie is input, running Basic Recommender\")\n",
    "  data = [(i[0], float(cossim(basic_vec, i[2])), i[1], i[4], i[3]) for i in chunks]\n",
    "  sim_df = spark.createDataFrame(data).toDF('movie_id', 'similarity', 'Title', 'Source', 'Plot')\n",
    "  sim_df = (sim_df.filter((sim_df.Title != input_user_1) & (sim_df.movie_id != input_user_1))\n",
    "            .dropDuplicates(['movie_id'])\n",
    "            .orderBy('similarity', ascending=False)\n",
    "            .limit(output_lim))\n",
    "  \n",
    "elif input_user_2: \n",
    "  \n",
    "  print(\"Two movies are input, running Advanced Recommender\")\n",
    "  data = [(i[0], float(cossim(combined_vec, i[2])), i[1], i[4], i[3]) for i in chunks]\n",
    "  sim_df = spark.createDataFrame(data).toDF('movie_id', 'similarity', 'Title', 'Source', 'Plot')\n",
    "  sim_df = (sim_df.filter((sim_df.Title != input_user_1) & (sim_df.movie_id != input_user_1)\n",
    "                          & (sim_df.Title != input_user_2) & (sim_df.movie_id != input_user_2))\n",
    "            .dropDuplicates(['movie_id'])\n",
    "            .orderBy('similarity', ascending=False)\n",
    "            .limit(output_lim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebcb6087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' # Example\\ntext1 = \"I love programming\"\\ntext2 = \"Coding is my passion\"\\nsimilarity_score = calculate_similarity(text1, text2)\\nprint(f\"Similarity: {similarity_score:.4f}\") '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize input, get output from BERT\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use mean pooling on the token embeddings to get sentence embeddings\n",
    "    sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze(dim=0)\n",
    "    return sentence_embedding\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    embedding1 = get_bert_embedding(text1)\n",
    "    embedding2 = get_bert_embedding(text2)\n",
    "    similarity = cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0)).item()\n",
    "    return similarity\n",
    "\n",
    "\"\"\" # Example\n",
    "text1 = \"I love programming\"\n",
    "text2 = \"Coding is my passion\"\n",
    "similarity_score = calculate_similarity(text1, text2)\n",
    "print(f\"Similarity: {similarity_score:.4f}\") \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebcab26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/10 16:29:45 WARN TaskSetManager: Stage 132 contains a task of very large size (7098 KiB). The maximum recommended task size is 1000 KiB.\n",
      "23/08/10 16:29:56 ERROR Executor: Exception in task 0.0 in stage 139.0 (TID 277)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/__init__.py\", line 30, in <module>\n",
      "    from .generic import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py\", line 27, in <module>\n",
      "    import numpy as np\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/__init__.py\", line 141, in <module>\n",
      "    from . import core\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/__init__.py\", line 51, in <module>\n",
      "    for envkey in env_added:\n",
      "ImportError: \n",
      "\n",
      "IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n",
      "\n",
      "Importing the numpy C-extensions failed. This error can happen for\n",
      "many reasons, often due to issues with your setup or how NumPy was\n",
      "installed.\n",
      "\n",
      "We have compiled some common reasons and troubleshooting tips at:\n",
      "\n",
      "    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n",
      "\n",
      "Please note and check the following:\n",
      "\n",
      "  * The Python version is: Python3.11 from \"/usr/local/bin/python3\"\n",
      "  * The NumPy version is: \"1.24.3\"\n",
      "\n",
      "and make sure that they are the versions you expect.\n",
      "Please carefully study the documentation linked above for further help.\n",
      "\n",
      "Original error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/08/10 16:29:56 WARN TaskSetManager: Lost task 0.0 in stage 139.0 (TID 277) (yus-air-2.lan executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/__init__.py\", line 30, in <module>\n",
      "    from .generic import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py\", line 27, in <module>\n",
      "    import numpy as np\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/__init__.py\", line 141, in <module>\n",
      "    from . import core\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/__init__.py\", line 51, in <module>\n",
      "    for envkey in env_added:\n",
      "ImportError: \n",
      "\n",
      "IMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n",
      "\n",
      "Importing the numpy C-extensions failed. This error can happen for\n",
      "many reasons, often due to issues with your setup or how NumPy was\n",
      "installed.\n",
      "\n",
      "We have compiled some common reasons and troubleshooting tips at:\n",
      "\n",
      "    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n",
      "\n",
      "Please note and check the following:\n",
      "\n",
      "  * The Python version is: Python3.11 from \"/usr/local/bin/python3\"\n",
      "  * The NumPy version is: \"1.24.3\"\n",
      "\n",
      "and make sure that they are the versions you expect.\n",
      "Please carefully study the documentation linked above for further help.\n",
      "\n",
      "Original error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n",
      "\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage4.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "23/08/10 16:29:56 ERROR TaskSetManager: Task 0 in stage 139.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/__init__.py\", line 26, in <module>\n    from . import dependency_versions_check\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n    from .utils.versions import require_version, require_version_core\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/__init__.py\", line 30, in <module>\n    from .generic import (\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py\", line 27, in <module>\n    import numpy as np\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/__init__.py\", line 141, in <module>\n    from . import core\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/__init__.py\", line 51, in <module>\n    for envkey in env_added:\nImportError: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"/usr/local/bin/python3\"\n  * The NumPy version is: \"1.24.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m calculate_similarity(text1, text2)\n\u001b[1;32m      9\u001b[0m sim_df \u001b[39m=\u001b[39m sim_df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39mBERT Score\u001b[39m\u001b[39m\"\u001b[39m, calculate_similarity_udf(col(\u001b[39m'\u001b[39m\u001b[39mPlot\u001b[39m\u001b[39m'\u001b[39m), lit(basic_movie_plot)))\n\u001b[0;32m---> 10\u001b[0m sim_df\u001b[39m.\u001b[39;49mshow(output_lim, truncate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/dataframe.py:912\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    905\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    906\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    909\u001b[0m         },\n\u001b[1;32m    910\u001b[0m     )\n\u001b[0;32m--> 912\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, int_truncate, vertical))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/__init__.py\", line 26, in <module>\n    from . import dependency_versions_check\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n    from .utils.versions import require_version, require_version_core\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/__init__.py\", line 30, in <module>\n    from .generic import (\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/utils/generic.py\", line 27, in <module>\n    import numpy as np\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/__init__.py\", line 141, in <module>\n    from . import core\n  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/__init__.py\", line 51, in <module>\n    for envkey in env_added:\nImportError: \n\nIMPORTANT: PLEASE READ THIS FOR ADVICE ON HOW TO SOLVE THIS ISSUE!\n\nImporting the numpy C-extensions failed. This error can happen for\nmany reasons, often due to issues with your setup or how NumPy was\ninstalled.\n\nWe have compiled some common reasons and troubleshooting tips at:\n\n    https://numpy.org/devdocs/user/troubleshooting-importerror.html\n\nPlease note and check the following:\n\n  * The Python version is: Python3.11 from \"/usr/local/bin/python3\"\n  * The NumPy version is: \"1.24.3\"\n\nand make sure that they are the versions you expect.\nPlease carefully study the documentation linked above for further help.\n\nOriginal error was: dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so, 0x0002): tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64')), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numpy/core/_multiarray_umath.cpython-311-darwin.so' (mach-o file, but is an incompatible architecture (have 'arm64', need 'x86_64'))\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "@udf(FloatType())\n",
    "def calculate_similarity_udf(text1, text2):\n",
    "    return calculate_similarity(text1, text2)\n",
    "\n",
    "sim_df = sim_df.withColumn(\"BERT Score\", calculate_similarity_udf(col('Plot'), lit(basic_movie_plot)))\n",
    "sim_df.show(output_lim, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f696f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
