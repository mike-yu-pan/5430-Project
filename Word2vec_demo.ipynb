{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adff3969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" !pip install -U pyspark\\nfrom pyspark.sql import SparkSession\\nimport os\\nimport sys\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable \""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" !pip install -U pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import sys\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e71aa4",
   "metadata": {},
   "source": [
    "# StreamQuest Movie Plot Searching System\n",
    "This application is created for movie studios to check the latest trends in the movie industry......"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f893e",
   "metadata": {},
   "source": [
    "## Section 1. Word2vec Movie Recommender Model Training\n",
    "The first part of this notebook is dedicated to data cleaning and trainning the word2vec model to create the following three tools for the studio writers and executives: \n",
    "1. Basic movie recommender: The user input one movie; and the system recommends 10 other movies with similar plotlines. \n",
    "2. Advance movie recommender: The user input two movies; and the system recommends 10 other movies with plotlines that are similar to the combination of these two movies. \n",
    "3. Duplicate plot checker: The user input his/her script for a new movie idea, and the system checks if his/her idea has already been produced in a previous movie. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11267477",
   "metadata": {},
   "source": [
    "### 1.1 Data cleaning\n",
    "There are three data sources used in this section: \n",
    "1. IMDB: used for matching movie name and ID\n",
    "2. Details: contains plot summaries, synopsis and movie ID, both summaries and synopsis will be used for trainning\n",
    "3. Wiki_Plot: contains plots and movie name, used for trainning\n",
    "\n",
    "Note that there might be multiple entries for a same movie because multiple authors might have written summaries for it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11a81153",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_imdb_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/title.basics.tsv.gz'\n",
    "path_to_plots_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/wiki_movie_plots_deduped.csv'\n",
    "path_to_details_dataset = '/Users/yupan/Library/CloudStorage/OneDrive-Personal/Academic/5430/data/IMDB_movie_details.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd85faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 09:55:18 WARN Utils: Your hostname, Yus-MacBook-Air-2.local resolves to a loopback address: 127.0.0.1; using 192.168.181.65 instead (on interface en0)\n",
      "23/08/11 09:55:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/08/11 09:55:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Apache Spark Version 3.4.1\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"Using Apache Spark Version\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055e8fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a total of  637758  movies in the imdb dataset\n",
      "there is a total of  2911  plot descriptions in the details dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joined dataset has  2857  entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------+--------------------+------------+\n",
      "|       id|         Title|                Plot|      Source|\n",
      "+---------+--------------+--------------------+------------+\n",
      "|tt2294449|22 Jump Street|Following their s...|imdb_details|\n",
      "|tt2294449|22 Jump Street|After making thei...|imdb_details|\n",
      "|tt0120623|  A Bug's Life|On a small island...|imdb_details|\n",
      "+---------+--------------+--------------------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# clean & combine the IMDB dataset with details dataset \n",
    "# reading the IMDB dataset\n",
    "imdb = spark.read.options(header = True, inferSchema = True, delimiter = \"\\t\")\\\n",
    "  .csv(path_to_imdb_dataset)\n",
    "# filter the imdb dataset so that only movies are included\n",
    "imdb = imdb.filter(\"titleType = 'movie'\")\\\n",
    "  .select('tconst', 'primaryTitle', 'startYear')\\\n",
    "    .withColumnRenamed('startYear', 'Year')\\\n",
    "      .withColumnRenamed('primaryTitle', 'Title')\\\n",
    "        .dropDuplicates(['Title', 'Year'])\n",
    "print('there is a total of ', imdb.count(), ' movies in the imdb dataset')\n",
    "\n",
    "\n",
    "# reading the details dataset, preserving only three important variables\n",
    "details_summary = spark.read.json(path_to_details_dataset)\n",
    "details_summary = details_summary\\\n",
    "  .select('movie_id','plot_summary')\\\n",
    "    .withColumnRenamed('plot_summary','Plot')\n",
    "\n",
    "# reading the details dataset, preserving only three important variables\n",
    "details_synopsis = spark.read.json(path_to_details_dataset)\n",
    "details_synopsis = details_synopsis.select('movie_id','plot_synopsis')\\\n",
    "  .filter(\"plot_synopsis != ''\")\\\n",
    "    .withColumnRenamed('plot_synopsis', 'Plot')\n",
    "\n",
    "details = details_summary.union(details_synopsis)\n",
    "print('there is a total of ', details.count(), ' plot descriptions in the details dataset')\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "# join the imdb with details by matching the unique identifier(e.g. tt0000000)\n",
    "imdb_join_details = imdb.join(details, imdb.tconst == details.movie_id, 'inner')\\\n",
    "  .withColumnRenamed('tconst', 'id')\\\n",
    "    .select('id', 'Title', 'Plot')\\\n",
    "      .withColumn(\"Source\", lit(\"imdb_details\"))\n",
    "\n",
    "print(\"The joined dataset has \", imdb_join_details.count(), \" entries\")\n",
    "# inspect the joined dataset\n",
    "imdb_join_details.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a785ac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there is a total of  33243  plot descriptions in the wiki_plot dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The joined dataset has  25361  entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 41:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+\n",
      "|       id|Title|                Plot|   Source|\n",
      "+---------+-----+--------------------+---------+\n",
      "|tt0790799|$9.99|The film mainly f...|wiki_plot|\n",
      "+---------+-----+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# clean and combine wiki plot dataset with imdb dataset\n",
    "from pyspark.sql.functions import length\n",
    "# reading the plot dataset, preserving only three important variables\n",
    "wiki_plot = spark.read.options(header = True, inferSchema = True, quote = '\"', escape = '\"', multiLine = True).csv(path_to_plots_dataset)\n",
    "wiki_plot = wiki_plot.select('Title', 'Release Year','Plot')\\\n",
    "  .withColumnRenamed('Release Year', 'Year')\\\n",
    "    .filter(length(wiki_plot['Plot']) >= 200) # filter out the very short plot descriptions\n",
    "print('there is a total of ', wiki_plot.count(), ' plot descriptions in the wiki_plot dataset')\n",
    "\n",
    "\n",
    "# join the imdb with the plot dataset by matching movie titles and release year\n",
    "imdb_join_plot = imdb.join(wiki_plot, [\"Title\", \"Year\"], 'inner')\\\n",
    "  .withColumnRenamed('tconst', 'id')\\\n",
    "    .select('id', 'Title', 'Plot')\\\n",
    "      .withColumn(\"Source\", lit(\"wiki_plot\"))\n",
    "\n",
    "print(\"The joined dataset has \", imdb_join_plot.count(), \" entries\")\n",
    "# inspect the joined dataset\n",
    "imdb_join_plot.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ad0ae58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merging & cleaning, there is a total of  28218  movie plot entries left in the merged dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+\n",
      "|       id|Title|                Plot|   Source|\n",
      "+---------+-----+--------------------+---------+\n",
      "|tt0790799|$9.99|The film mainly f...|wiki_plot|\n",
      "+---------+-----+--------------------+---------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# combine the above two dataset to get the dataset that we will train the model on\n",
    "df = imdb_join_plot.union(imdb_join_details)\n",
    "\n",
    "print('after merging & cleaning, there is a total of ', df.count(), ' movie plot entries left in the merged dataset')\n",
    "# inspect the combined new dataset\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf51036",
   "metadata": {},
   "source": [
    "### 1.2 Trainning Word2vec Model \n",
    "This Word2Vec model is trained using the cleaned data above, the model is fed with around 28,000 entries of texts that describe movie plots. The resulting model will be useful in finding similarities in movie plotlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5809ac58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|       id|Title|                Plot|   Source|           inputText|              tokens|   tokens_sw_removed|\n",
      "+---------+-----+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "|tt0790799|$9.99|The film mainly f...|wiki_plot|The film mainly f...|[the, film, mainl...|[film, mainly, fo...|\n",
      "+---------+-----+--------------------+---------+--------------------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# tokenize and remove stop words in this cell\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, Word2Vec\n",
    "\n",
    "# create a new field by copying Plot\n",
    "df = df.withColumn('inputText', F.col('Plot')) \n",
    "\n",
    "# regular expression tokenizer to tokenize inputText into individual tokens (words)\n",
    "regextok = RegexTokenizer(gaps = False, pattern = '\\w+', inputCol = 'inputText', outputCol = 'tokens')\n",
    "\n",
    "# StopWordsRemover to remove stopwords in the list of tokens\n",
    "stopwrmv = StopWordsRemover(inputCol = 'tokens', outputCol = 'tokens_sw_removed')\n",
    "df = regextok.transform(df)\n",
    "df = stopwrmv.transform(df)\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7e668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/11 09:58:31 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train word2vec model, the parameters here can be changed to optimize the model\n",
    "word2vec = Word2Vec(vectorSize = 100, minCount = 5, inputCol = 'tokens_sw_removed', outputCol = 'wordvectors')\n",
    "model = word2vec.fit(df)\n",
    "\n",
    "# using transform to add wordvectors column to dataframe\n",
    "df = model.transform(df)\n",
    "chunks = df.select('id', 'Title','wordvectors', 'Plot', 'Source').limit(30000).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a433ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to calculate cosine similarity for later\n",
    "import numpy as np\n",
    "def cossim(v1, v2): \n",
    "  '''\n",
    "      cossim(v1, v2) calculates the cosine similarity between v1 and v1.\n",
    "      If v1 or v2 is a zero vector, it will return 0\n",
    "  '''\n",
    "  if np.dot(v1, v1) == 0 or np.dot(v2, v2) == 0:\n",
    "      return 0.0\n",
    "  return float(np.dot(v1, v2) / np.sqrt(np.dot(v1, v1)) / (np.sqrt(np.dot(v2, v2))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b98e1a0",
   "metadata": {},
   "source": [
    "### 1.3 Create Basic, Advanced Recommender and Duplicate Plot Checker\n",
    "These three tools use the same model at their core. Therefore, to optimize performance, the implementation will create only a single session when running. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd10102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a function to obtain the plot string from the plot dataset\n",
    "def acquire_plot(base_movie: str): \n",
    "  # input: a movie name (precise) or a movie id \n",
    "  # output: the movie's plot\n",
    "\n",
    "  if base_movie.startswith(\"tt\"):   # search by movie name\n",
    "    base_movie_row = df.filter(df.id == base_movie).collect()\n",
    "  else:                             # search by movie id\n",
    "    base_movie_row = df.filter(df.Title == base_movie).collect()\n",
    "\n",
    "  if base_movie_row: \n",
    "    movie_plot = base_movie_row[0]['Plot']\n",
    "    return movie_plot\n",
    "  else: \n",
    "    print(\"Sorry, \", base_movie, \" is not found in the database. Please type in exact movie names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4973a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_preprocessing(plot: str): \n",
    "  plot_df = spark.createDataFrame([(1, plot)]).toDF('index','inputText')\n",
    "  plot_tok = regextok.transform(plot_df)\n",
    "  plot_swr = stopwrmv.transform(plot_tok)\n",
    "  plot_vec = model.transform(plot_swr)\n",
    "  plot_vec = plot_vec.select('wordvectors').collect()[0][0]\n",
    "  return plot_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb45e02",
   "metadata": {},
   "source": [
    "### 1.3.1 Basic recommender\n",
    "The Basic Recommender only takes in one movie as the only parameter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "717173d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_user_1 = '2012'    # User input The exact movie name or movie id(e.g. 'tt1023003')\n",
    "basic_movie_plot = acquire_plot(input_user_1)\n",
    "basic_vec = query_preprocessing(basic_movie_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b538bcc",
   "metadata": {},
   "source": [
    "### 1.3.2 Advanced Recommender\n",
    "The Advanced Recommender takes in one extra movie as the second parameter, then our word2vec model will be able to recommend a third movie that has similar plot as the combination of the first two. : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39f4126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user_2 = '' #'tt0468569'    # User input The exact movie name or movie id(e.g. 'tt1023003')\n",
    "if input_user_2: \n",
    "  second_movie_plot = acquire_plot(input_user_2)\n",
    "  second_vec = query_preprocessing(second_movie_plot)\n",
    "  combined_vec = basic_vec + second_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678c02b5",
   "metadata": {},
   "source": [
    "### 1.3.3 Duplicate Plot Checker\n",
    "The duplicate Plot checker is similar to the basic recommender where it finds an existing movie with similar plot as the user's input plot description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ed4012a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user_3 = '' #'Once upon a time in a crime ridden Gotham City, a member of the rich Wayne family decided to put on a mask and protect the people of Gotham'\n",
    "check_vec = query_preprocessing(input_user_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13ed06",
   "metadata": {},
   "source": [
    "### 1.4 implementation of the above three tools\n",
    "The parameters of the three tools have been created. To optimize performance, the implementation will create only one single session when running. \n",
    "\n",
    "__Note for Meenu__ : the order of the IF clauses in this following cell is important, the user can only run one of the three tools at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c628e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only one movie is input, running Basic Recommender\n"
     ]
    }
   ],
   "source": [
    "output_lim = 10\n",
    "\n",
    "if input_user_3: \n",
    "  print(\"Running Duplicate Plot Checker\")\n",
    "  data = [(i[0], float(cossim(check_vec, i[2])), i[1], i[4], i[3]) for i in chunks]\n",
    "  sim_df = spark.createDataFrame(data).toDF('movie_id', 'similarity', 'Title', 'Source', 'Plot')\n",
    "  sim_df = (sim_df.dropDuplicates(['movie_id'])\n",
    "            .orderBy('similarity', ascending=False)\n",
    "            .limit(output_lim))\n",
    "\n",
    "elif not input_user_2:  # if input_user_2 is empty, then run the basic recommender\n",
    "  \n",
    "  print(\"Only one movie is input, running Basic Recommender\")\n",
    "  data = [(i[0], float(cossim(basic_vec, i[2])), i[1], i[4], i[3]) for i in chunks]\n",
    "  sim_df = spark.createDataFrame(data).toDF('movie_id', 'similarity', 'Title', 'Source', 'Plot')\n",
    "  sim_df = (sim_df.filter((sim_df.Title != input_user_1) & (sim_df.movie_id != input_user_1))\n",
    "            .dropDuplicates(['movie_id'])\n",
    "            .orderBy('similarity', ascending=False)\n",
    "            .limit(output_lim))\n",
    "  \n",
    "elif input_user_2: \n",
    "  \n",
    "  print(\"Two movies are input, running Advanced Recommender\")\n",
    "  data = [(i[0], float(cossim(combined_vec, i[2])), i[1], i[4], i[3]) for i in chunks]\n",
    "  sim_df = spark.createDataFrame(data).toDF('movie_id', 'similarity', 'Title', 'Source', 'Plot')\n",
    "  sim_df = (sim_df.filter((sim_df.Title != input_user_1) & (sim_df.movie_id != input_user_1)\n",
    "                          & (sim_df.Title != input_user_2) & (sim_df.movie_id != input_user_2))\n",
    "            .dropDuplicates(['movie_id'])\n",
    "            .orderBy('similarity', ascending=False)\n",
    "            .limit(output_lim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebcb6087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embedding(text):\n",
    "    # Tokenize input, get output from BERT\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use mean pooling on the token embeddings to get sentence embeddings\n",
    "    sentence_embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze(dim=0)\n",
    "    return sentence_embedding\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    embedding1 = get_bert_embedding(text1)\n",
    "    embedding2 = get_bert_embedding(text2)\n",
    "    similarity = cosine_similarity(embedding1.unsqueeze(0), embedding2.unsqueeze(0)).item()\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62794a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "accuracy_df = sim_df.toPandas()\n",
    "def compute_scores(row):\n",
    "    bert_score = calculate_similarity(row['Plot'], basic_movie_plot)\n",
    "    bert_accuracy = (row['similarity'] - bert_score)*100 / bert_score\n",
    "    return bert_score, bert_accuracy\n",
    "\n",
    "accuracy_df[['BERT_score', 'BERT_delta%']] = accuracy_df.apply(lambda row: compute_scores(row), axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1161a04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>similarity</th>\n",
       "      <th>Title</th>\n",
       "      <th>Source</th>\n",
       "      <th>Plot</th>\n",
       "      <th>BERT_score</th>\n",
       "      <th>BERT_delta%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0319262</td>\n",
       "      <td>0.943857</td>\n",
       "      <td>The Day After Tomorrow</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>Jack Hall, an American paleoclimatologist, and...</td>\n",
       "      <td>0.963338</td>\n",
       "      <td>-2.022219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0377062</td>\n",
       "      <td>0.934648</td>\n",
       "      <td>Flight of the Phoenix</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>When an Amacore oil rig in the Gobi Desert of ...</td>\n",
       "      <td>0.944732</td>\n",
       "      <td>-1.067381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0082810</td>\n",
       "      <td>0.929810</td>\n",
       "      <td>Night Crossing</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>The film opens with a brief summary of 1961's ...</td>\n",
       "      <td>0.933364</td>\n",
       "      <td>-0.380793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0120647</td>\n",
       "      <td>0.927826</td>\n",
       "      <td>Deep Impact</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>On May 10, 1998, teenaged amateur astronomer L...</td>\n",
       "      <td>0.962801</td>\n",
       "      <td>-3.632629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt4363250</td>\n",
       "      <td>0.927003</td>\n",
       "      <td>Lost in the Pacific</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>The year is 2020, two armed soldiers pass thro...</td>\n",
       "      <td>0.928369</td>\n",
       "      <td>-0.147097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tt0088172</td>\n",
       "      <td>0.923809</td>\n",
       "      <td>Starman</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>Launched in 1977, the Voyager 2 space probe ca...</td>\n",
       "      <td>0.935747</td>\n",
       "      <td>-1.275793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tt1470827</td>\n",
       "      <td>0.920738</td>\n",
       "      <td>Monsters</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>After a NASA deep-space probe (sent to verify ...</td>\n",
       "      <td>0.945731</td>\n",
       "      <td>-2.642732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tt1860357</td>\n",
       "      <td>0.916612</td>\n",
       "      <td>Deepwater Horizon</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>On April 20, 2010, Deepwater Horizon, an oil d...</td>\n",
       "      <td>0.922323</td>\n",
       "      <td>-0.619277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tt0053137</td>\n",
       "      <td>0.915722</td>\n",
       "      <td>On the Beach</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>In early 1964, in the months following World W...</td>\n",
       "      <td>0.948907</td>\n",
       "      <td>-3.497158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tt0119567</td>\n",
       "      <td>0.915631</td>\n",
       "      <td>The Lost World: Jurassic Park</td>\n",
       "      <td>wiki_plot</td>\n",
       "      <td>On Isla Sorna, an island off the Costa Rican c...</td>\n",
       "      <td>0.946503</td>\n",
       "      <td>-3.261696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    movie_id  similarity                          Title     Source   \n",
       "0  tt0319262    0.943857         The Day After Tomorrow  wiki_plot  \\\n",
       "1  tt0377062    0.934648          Flight of the Phoenix  wiki_plot   \n",
       "2  tt0082810    0.929810                 Night Crossing  wiki_plot   \n",
       "3  tt0120647    0.927826                    Deep Impact  wiki_plot   \n",
       "4  tt4363250    0.927003            Lost in the Pacific  wiki_plot   \n",
       "5  tt0088172    0.923809                        Starman  wiki_plot   \n",
       "6  tt1470827    0.920738                       Monsters  wiki_plot   \n",
       "7  tt1860357    0.916612              Deepwater Horizon  wiki_plot   \n",
       "8  tt0053137    0.915722                   On the Beach  wiki_plot   \n",
       "9  tt0119567    0.915631  The Lost World: Jurassic Park  wiki_plot   \n",
       "\n",
       "                                                Plot  BERT_score  BERT_delta%  \n",
       "0  Jack Hall, an American paleoclimatologist, and...    0.963338    -2.022219  \n",
       "1  When an Amacore oil rig in the Gobi Desert of ...    0.944732    -1.067381  \n",
       "2  The film opens with a brief summary of 1961's ...    0.933364    -0.380793  \n",
       "3  On May 10, 1998, teenaged amateur astronomer L...    0.962801    -3.632629  \n",
       "4  The year is 2020, two armed soldiers pass thro...    0.928369    -0.147097  \n",
       "5  Launched in 1977, the Voyager 2 space probe ca...    0.935747    -1.275793  \n",
       "6  After a NASA deep-space probe (sent to verify ...    0.945731    -2.642732  \n",
       "7  On April 20, 2010, Deepwater Horizon, an oil d...    0.922323    -0.619277  \n",
       "8  In early 1964, in the months following World W...    0.948907    -3.497158  \n",
       "9  On Isla Sorna, an island off the Costa Rican c...    0.946503    -3.261696  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74bf7433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.8546775057540987"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_df['BERT_delta%'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcab26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "@udf(FloatType())\n",
    "def calculate_similarity_udf(text1, text2):\n",
    "    return calculate_similarity(text1, text2)\n",
    "\n",
    "sim_df = sim_df.withColumn(\"BERT Score\", calculate_similarity_udf(col('Plot'), lit(basic_movie_plot)))\n",
    "sim_df.show(output_lim, truncate=False) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f696f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
